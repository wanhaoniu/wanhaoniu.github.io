<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wanhao Niu - Personal Homepage</title>
    <style>
        .about-me-container {
            display: flex;
            align-items: flex-start;
        }
        .about-me-text {
            margin-right: 100px;
        }
        img.publication-image {
            width: 300px;
            height: auto;
            margin-right: 10px;
        }
        .publication-entry {
            display: flex;
            align-items: center;
            margin-bottom: 20px;
        }
        .date {
            font-family: monospace;
            font-size: 16px;
        }
    </style>
</head>
<body>
    <section class="about-me-container">
        <div class="about-me-text">
            <h2>About Me</h2>
            <p>Hello, I'm Wanhao Niu, currently pursuing a Master's degree in Mechanical Engineering at Shanghai Jiao Tong University under the supervision of Prof. Chungang Zhuang, expected to graduate in June 2025. In 2024, I spent five months at Tokyo Institute of Technology under the guidance of Prof. Masaki Yamakita during an exchange program called YSEP. My research focuses on AI-driven model-free object grasping in unstructured environments, and I have extensive experience in robotic grasping and deep learning. I have received the National Scholarship and was recognized for my excellent undergraduate thesis (top 1%). Additionally, I serve as a reviewer for ICRA. I am planning to apply for a PhD program in 2025 to further advance my research in this field.</p>
            <p>If you'd like to get in touch, please email me at <a href="mailto:nwh1093412390@sjtu.edu.cn">nwh1093412390@sjtu.edu.cn</a>.</p>
            <p>Please find my resume <a href="CV Wanhao Niu.pdf">here</a>; my undergraduate transcript <a href="Undergraduate Transcript Wanhao Niu.pdf">here</a>; and my graduate transcript <a href="Graduate Transcript Wanhao Niu.pdf">here</a>.</p>
            <p>You can also find a brief research proposal <a href="Research Proposal Wanhao Niu.pdf">here</a>, but it is only on the draft stage.</p>
        </div>
        <img src="pic/profile.jpg" alt="Wanhao Niu's Photo" style="width:400px;height:auto;">
    </section>
    <section>
        <h2>Education</h2>
        <ul>
            <li><strong class="date">2018.09 - 2022.07:</strong> Xi'an Jiao Tong University - Bachelor's degree in Mechanical Engineering (GPA: 90.19, Rank: 5/205)</li>
            <li><strong class="date">2022.09 - 2025.06:</strong> Shanghai Jiao Tong University - Master's degree in Mechanical Engineering (GPA: 3.72)</li>
            <li><strong class="date">2024.04 - 2024.09:</strong> Tokyo Institute of Technology - YSEP Exchange Student (Research: Model-Free Object Grasping Technology Driven by Artificial Intelligence)</li>
        </ul>
    </section>
    <section>
        <h2>Research Interests</h2>
        <ul>
            <li>Model-Free Object Grasping Technology; Robotic Grasping; Deep Learning; Graph Convolutional Networks</li>
        </ul>
    </section>
    <section>
        <h2>Publications</h2>
        <h3>Journal Papers</h3>
        <div class="publication-entry">
            <img src="pic/EAAI.jpg" alt="Journal Paper 1 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1016/j.engappai.2024.109320"><strong>Niu W.</strong>, Zhu Z., Wang H., Zhuang C. - Customizable 6 Degrees of Freedom Grasping Dataset and an Interactive Training Method for GCN (Engineering Application of Artificial Intelligence)</a>  <a href="https://github.com/cgzhuang/CGra-D">Code</a>
                <p>We introduce an innovative method for generating robotic grasping datasets in simulated environments without manual annotations—utilizing customizable gripper movements and detailed evaluation metrics—to enhance dataset diversity and applicability; additionally, we present an end-to-end grasp prediction network leveraging advanced graph convolution techniques and a novel interactive training method that together improve model performance and training efficiency in robotic grasping applications.</p>
            </div>
        </div>
        <div class="publication-entry">
            <img src="pic/IEEETCDS.jpg" alt="Journal Paper 2 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1109/TCDS.2024.3403900"><strong>Niu W.</strong>, Wang H., Zhuang C. - Adaptive Multiview Graph Convolutional Network for 3D Point Cloud Classification and Segmentation (IEEE Transactions on Cognitive and Developmental Systems)</a> <a href="https://github.com/cgzhuang/AM-GCN">Code</a>
                <p>We propose an adaptive multiview graph convolutional network (AM-GCN) that integrates global geometric features and local features from multiple view projections of point clouds to address the limitation of existing methods that overlook long-distance geometric relationships, thereby enhancing point cloud classification and segmentation accuracy with competitive performance across multiple datasets.</p>
            </div>
        </div>
        <div class="publication-entry">
            <img src="pic/ASME.jpg" alt="Journal Paper 3 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1115/1.4066281">Zhuang C., <strong>Niu W.</strong>, Wang H. - Sparse Convolution-Based 6D Pose Estimation for Robotic Bin-Picking With Point Clouds (Journal of Mechanisms and Robotics)</a> <a href="https://github.com/cgzhuang/Sparse-convolution-based-6D-pose-estimation-for-robotic-bin-picking-with-point-clouds">Code</a>
                <p>We introduce a novel two-stage neural network that processes point clouds to predict 6D object poses for robotic bin-picking tasks, effectively overcoming challenges of occlusion and random stacking, outperforming baseline methods in both segmentation accuracy and pose refinement, and demonstrating efficacy in real-world industrial applications.</p>
            </div>
        </div>
        <div class="publication-entry">
            <img src="pic/RCIM.jpg" alt="Journal Paper 4 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1016/j.rcim.2024.102879">Zhuang C., Wang H., <strong>Niu W.</strong>, Han D. - A Parallel Graph Network for Generating 7-DoF Model-free Grasps in Unstructured Scenes Using Point Cloud (Robotics and Computer-Integrated Manufacturing)</a>
                <p>We propose a parallelized graph-based pipeline for 7-DoF grasp pose generation from point clouds that simultaneously performs feature embedding and grasp location focusing in two branches using a positional attention-based graph convolution operator, resulting in improved grasp predictions that outperform benchmarks on the GraspNet-1Billion dataset and achieve high success rates in real-world robotic grasping of unknown objects.</p>
            </div>
        </div>
        <h3>Conference Papers</h3>
        <div class="publication-entry">
            <img src="pic/ICMRE.jpg" alt="Conference Paper 1 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1109/ICMRE56789.2023.10106576"><strong>Niu W.</strong>, Wang H., Zhuang C. - Detection for Tiny Screw and Screw Hole by Semantic Segmentation Model (ICMRE 2023)</a>
                <p>We propose a method combining semantic segmentation models with post-processing to improve the detection of screws and screw holes on mobile phone PCBs, achieving 99.7% accuracy even in vibrating conditions, thereby enhancing the stability and efficiency of automatic assembly and disassembly on production lines.</p>
            </div>
        </div>
        <div class="publication-entry">
            <img src="pic/IROS.jpg" alt="Conference Paper 2 Image" class="publication-image">
            <div>
                <a href="https://doi.org/10.1109/IROS55552.2023.10341549">Wang H., <strong>Niu W.</strong>, Zhuang C. - GraNet: A Multi-level Graph Network for 6-DoF Grasp Generation in Cluttered Scenes (IROS 2023)</a> <a href="https://github.com/wanhaoniu/GraNet">Code</a>
                <p>We propose GraNet, a graph-based grasp pose generation framework that translates point cloud scenes into multi-level graphs to enhance feature embedding and improve 6-DoF object-agnostic grasping in unstructured environments, achieving state-of-the-art performance and higher grasp success rates, especially on unseen objects.</p>
            </div>
        </div>
    </section>
    <section>
        <h2>Honors & Awards</h2>
        <ul>
            <li>JASSO Scholarship, Tokyo Institute of Technology (April 2024)</li>
            <li>Graduate National Scholarship, Shanghai Jiao Tong University (September 2023)</li>
            <li>Excellent Undergraduate Thesis (Top 1%), Xi'an Jiao Tong University (July 2022)</li>
            <li>Outstanding Graduate Student of Shaanxi Province, Xi'an Jiao Tong University (July 2022)</li>
            <li>First Prize: Challenge Cup National Competition, Xi'an Jiao Tong University (April 2022)</li>
            <li>Shangyin Elite Student Scholarship, Xi'an Jiao Tong University (September 2021)</li>
        </ul>
    </section>
    <section>
        <h2>Professional Skills</h2>
        <ul>
            <li>Hardware: UR Robot Platform, PhotoNeo Industrial Camera, Signal Generator</li>
            <li>Software: VSCode, MATLAB, AutoCAD, SolidWorks, COMSOL</li>
            <li>Programming Languages: Python (Proficient), MATLAB (Basic), C++ (Basic)</li>
            <li>Frameworks: PyTorch (Proficient), OpenCV (Proficient), PyBullet (Proficient), TensorFlow (Basic), Issac Lab (Basic)</li>
        </ul>
    </section>
    <section>
        <h2>Languages</h2>
        <ul>
            <li>Mandarin Chinese (Native)</li>
            <li>English (Fluent - IELTS 6.5)</li>
        </ul>
    </section>
    <section>
        <h2>Hobbies & Interests</h2>
        <ul>
            <li>City Walk; Table Tennis; Photography; Traveling and Cultural Exchange</li>
        </ul>
    </section>    
</body>
</html>
